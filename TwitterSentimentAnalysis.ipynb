{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries \n",
    "\n",
    "\n",
    "from datetime import date\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob  \n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scraping tweets \n",
    "<br>The code below scrapes 1000 tweets with the keyword ‘crypto’. The tweets list is created is converted into a data frame with the column name Tweets.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating datetime objects\n",
    "from datetime import datetime, timedelta\n",
    "now = datetime.now()\n",
    "now = now.strftime('%Y-%m-%d')\n",
    "yesterday = datetime.now() - timedelta(days = 1)\n",
    "yesterday = yesterday.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = input('Enter a topic or keyword, please:')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list to append tweets \n",
    "\n",
    "import csv\n",
    "\n",
    "tweet_list = []\n",
    "maxTweets = 1000 #Open/create a file to append data to\n",
    "\n",
    "\n",
    "csvFile = open(keyword +'-sentiment-' + now + '.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id','date','tweet',])\n",
    "\n",
    "#use tweetsearchscraper to append tweets \n",
    "\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + ' lang:en since:' +  yesterday + ' until:' + now + ' -filter:links -filter:replies').get_items()):\n",
    "        if i > maxTweets :\n",
    "            break\n",
    "        csvWriter.writerow([tweet.id, tweet.date, tweet.content])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv(keyword +'-sentiment-' + now + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cleaning the tweets\n",
    "\n",
    "<br>We create a function to clean the tweets. We use regex (regular expressions) to remove @mentions, #hastags, hyperlinks, retweets, and many more. Finally, we apply the function to our tweets_to_df data frame and create a new column for the cleaned tweets<Br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  cleanTweets(text):\n",
    "    \n",
    "    text = re.sub('@[A-Za-z0-9_]+', '', text) #removes @mentions\n",
    "    text = re.sub('#','',text) #removes hastag '#' symbol\n",
    "    text = re.sub('RT[\\s]+','',text)\n",
    "    text = re.sub('https?:\\/\\/\\S+', '', text) \n",
    "    text = re.sub('\\n',' ',text)\n",
    "    return text\n",
    "\n",
    "\n",
    "tweet_df[\"cleaned_tweets \"] = tweet_df[\"tweet\"].apply(cleanTweets)\n",
    "\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.to_csv('tweetsSentiment.csv') #write dataframe into csv file\n",
    "savedTweets = pd.read_csv('tweetsSentiment.csv',index_col=0) #reads csv file\n",
    "savedTweets.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Detect sentiments\n",
    "\n",
    "<br> We create a function that gets the subjectivity and polarity of each tweet and saves them to new columns with the names Subjectivity and Polarity respectively.Using NLTK , \n",
    "\n",
    "<br>If a tweet has high subjectivity i.e. close to 1, it means the tweet contains more of a personal opinion than factual information.\n",
    "\n",
    "<br>\n",
    "The polarity score lies between (-1 to 1) where -1 identifies the most negative words and 1 identifies the most positive words <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedTweets['cleaned_tweets ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Create a function to get the polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedTweets['Subjectivity'] = savedTweets['cleaned_tweets '].apply(getSubjectivity)\n",
    "savedTweets['polarity'] = savedTweets['cleaned_tweets '].apply(getPolarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DropThe Unwanted Column\n",
    "\n",
    "savedTweets = savedTweets.drop(\"tweet\", axis = 1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets  determine if a tweet’s polarity is positive, neutral, or negative using a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check negative, neutral and positive analysis\n",
    "\n",
    "def getAnalysis(score):\n",
    "    if score<0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "\n",
    "savedTweets['Analysis'] = savedTweets['polarity'].apply(getAnalysis)\n",
    "\n",
    "savedTweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we count the total number of tweets for each polarity\n",
    "\n",
    "savedTweets['Analysis'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polarity graph to show sentiment distribution\n",
    "\n",
    "from matplotlib.pyplot import colorbar\n",
    "from seaborn import barplot\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize= (7,5))\n",
    "savedTweets['Analysis'].value_counts().plot(kind = 'bar', colormap='Paired' )\n",
    "#color = ['red','blue','orange']\n",
    "plt.title(\"Sentiment distribution\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.xlabel(\"Polarity\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pie chart to show percentage distribution of polarity\n",
    "#pie chart to show percentage distribution of polarity\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "colors = ('green', 'grey', 'red')\n",
    "wp={'linewidth':2, 'edgecolor': 'black'}\n",
    "tags=savedTweets['Analysis'].value_counts()\n",
    "explode = (0.1,0.1,0.1)\n",
    "tags.plot(kind='pie', autopct='%1.1f%%', shadow=True, colors=colors, \n",
    "         startangle=90, wedgeprops=wp, explode=explode, label='')\n",
    "plt.title('Distribution of polarity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the subjectivity and polarity on a scatter diagram\n",
    "\n",
    "from turtle import color\n",
    "\n",
    "\n",
    "plt.figure(figsize = (7,7))\n",
    "for i in range (0,savedTweets.shape[0]):\n",
    "    plt.scatter(savedTweets['polarity'],savedTweets['Subjectivity'],color = 'orange')\n",
    "\n",
    "plt.title = 'subjectivity and polarity on a scatter diagram'\n",
    "plt.xlabel (\"Polarity\")\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Creating a word cloud for the tweets\n",
    "\n",
    "<br>To understand which words have been used most in the tweets, we create a word cloud function for both positive and negative tweets.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(text):\n",
    "    allwords = ''.join([tweets for tweets in text ])\n",
    "    wordCloud = WordCloud(background_color= 'white',height= 700,width= 600 ,random_state= 30 ,max_font_size= 120).generate(allwords)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(wordCloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "tweets = savedTweets['cleaned_tweets ']\n",
    "word_cloud(tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedTweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create wordcloud for positive tweets\n",
    "positiveTweets = savedTweets.loc[savedTweets['Analysis']== 'Positive','tweets']\n",
    "word_cloud(positiveTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveTweets = savedTweets.loc[savedTweets['Analysis']== Negative, \"cleaned_tweets\"].item()\n",
    "word_cloud(positiveTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the percentage of positive tweets\n",
    "pTweets = savedTweets[savedTweets['Analysis']=='Positive']\n",
    "pTweets = pTweets['cleaned_tweets']\n",
    "percentage = round((pTweets.shape[0]/savedTweets.shape[0]) *100,1)\n",
    "print('Percentage of positive tweets: {0}%'.format(percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the percentage of negative tweets\n",
    "nTweets = savedTweets[savedTweets['Analysis']=='Negative']\n",
    "nTweets = nTweets['cleanedTweets']\n",
    "percentage = round((nTweets.shape[0]/savedTweets.shape[0]) *100,1)\n",
    "print('Percentage of negative tweets: {0}%'.format(percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break each tweet sentence into words\n",
    "sentences = []\n",
    "for word in savedTweets['cleaned_tweets']:\n",
    "    sentences.append(word)\n",
    "sentences\n",
    "\n",
    "lines = list()\n",
    "for line in sentences:\n",
    "    words = line.split()\n",
    "    for w in words:\n",
    "        lines.append(w)\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming all the words to their root word\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stem=[]\n",
    "for word in lines:\n",
    "    stem.append(stemmer.stem(word))\n",
    "stem[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove stop words which are the common words used in the English Language such as ‘on’, ‘the’, ‘is’ etc. We then group the rest together to their root words eg joined, joining, and joint are grouped together as a single word — join and save it to a new data frame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming all the words to their root word\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stem=[]\n",
    "for word in lines:\n",
    "    stem.append(stemmer.stem(word))\n",
    "stem[:20]\n",
    "#removes stopwords (very common words in a sentence)\n",
    "stem2 = []\n",
    "for word in stem:\n",
    "    if word not in nlp.Defaults.stop_words:\n",
    "        stem2.append(word)\n",
    "#creates a new dataframe for the stem and shows the count of the most used words\n",
    "df = pd.DataFrame(stem2)\n",
    "df=df[0].value_counts()\n",
    "df #shows the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes stopwords (very common words in a sentence)\n",
    "stem2 = []\n",
    "for word in stem:\n",
    "    if word not in nlp.Defaults.stop_words:\n",
    "        stem2.append(word)\n",
    "#stem2[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a new dataframe for the stem\n",
    "df = pd.DataFrame(stem2)\n",
    "df=df[0].value_counts()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots the top 20 used words\n",
    "df = df[:20]\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(df.values, df.index, alpha=0.8)\n",
    "plt.title('Top Words Overall')\n",
    "plt.xlabel('Count of words', fontsize=12)\n",
    "plt.ylabel('Words from Tweets', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a2b1ae56fcd46c2523e87dff47e23dfc5e79f59c13d5521028cc5ca56e00fbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
